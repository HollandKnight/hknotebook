{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "![alt text](https://s3.amazonaws.com/blaze4/HK+logo.png)\n",
    "\n",
    "# *LIBOR TRANSITION ASSISTANT (LTA)*\n",
    "\n",
    "Holland & Knight's LIBOR Transition Assistant (LTA) can significantly reduce the time and cost ordinarily associated with portfolio review projects like those being undertaken ahead of the discontinuance of LIBOR. Many of the solutions offered by other professional firms are little more than white-labeled platforms originally designed to tackle different tasks. In contrast, LTA was built from the ground up to assist financial institutions in the transition away from LIBOR. The LTA is the product of Holland & Knight's own data scientists, technologists and subject matter experts, and was developed using state-of-the-art open source machine learning algorithms.  As a result, we are constantly improving our LTA solution, and where necessary, maintaining the ability to tailor it to an individual lender's specific loan portfolio, workflows and internal systems.    \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-02T12:30:44.239904Z",
     "start_time": "2019-06-02T08:55:38.989Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.23) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0620 23:19:34.841624 4672591296 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /Users/josiasdewey/miniconda3/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /Users/josiasdewey/miniconda3/lib/python3.7/site-packages (from bert-tensorflow) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "!pip install bert-tensorflow\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re,sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import auth as google_auth\n",
    "    google_auth.authenticate_user()\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] =  'elite-mix-229322-5ae3dc690187.json'\n",
    "#service = googleapiclient.discovery.build('ml', 'v1')\n",
    "\n",
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "\n",
    "OUTPUT_DIR = ''#@param {type:\"string\"}\n",
    "#@markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = False #@param {type:\"boolean\"}\n",
    "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
    "USE_BUCKET = True #@param {type:\"boolean\"}\n",
    "BUCKET_HOTDOG = 'bert_hk3' #@param {type:\"string\"}\n",
    "\n",
    "if USE_BUCKET:\n",
    "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET_HOTDOG, OUTPUT_DIR)\n",
    "\n",
    "\n",
    "if DO_DELETE:\n",
    "  try:\n",
    "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "  except:\n",
    "    # Doesn't matter if the directory didn't exist\n",
    "    pass\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
    "\n",
    "train_hotdog = pd.read_excel('libor_train.xlsx')\n",
    "test_hotdog = pd.read_excel('libor_test.xlsx')\n",
    "train_hotdog = train_hotdog.sample(500)\n",
    "test_hotdog = test_hotdog.sample(160)\n",
    "train_hotdog.columns\n",
    "DATA_COLUMN_HOT = 'Text'\n",
    "LABEL_COLUMN_HOT = 'category_id'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list_hot = [0, 1]\n",
    "\n",
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples_hot = train_hotdog.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN_HOT], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN_HOT]), axis = 1)\n",
    "\n",
    "test_InputExamples_hot = test_hotdog.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN_HOT], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN_HOT]), axis = 1)\n",
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "        return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features_hot = bert.run_classifier.convert_examples_to_features(train_InputExamples_hot, label_list_hot, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features_hot = bert.run_classifier.convert_examples_to_features(test_InputExamples_hot, label_list_hot, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "    bert_module = hub.Module(\n",
    "        BERT_MODEL_HUB,\n",
    "        trainable=True)\n",
    "    bert_inputs = dict(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "        inputs=bert_inputs,\n",
    "        signature=\"tokens\",\n",
    "        as_dict=True)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for politeness data.\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "\n",
    "        # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "\n",
    "        # If we're train/eval, compute loss between predicted and actual label\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)\n",
    "\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        f1_score = tf.contrib.metrics.f1_score(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        auc = tf.metrics.auc(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        recall = tf.metrics.recall(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        precision = tf.metrics.precision(\n",
    "            label_ids,\n",
    "            predicted_labels) \n",
    "        true_pos = tf.metrics.true_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        true_neg = tf.metrics.true_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)   \n",
    "        false_pos = tf.metrics.false_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)  \n",
    "        false_neg = tf.metrics.false_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"true_positives\": true_pos,\n",
    "            \"true_negatives\": true_neg,\n",
    "            \"false_positives\": false_pos,\n",
    "            \"false_negatives\": false_neg\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn\n",
    "\n",
    "# Compute train and warmup steps from batch size\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features_hot) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list_hot),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator_hot = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})\n",
    "\n",
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features_hot,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)\n",
    "\n",
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator_hot.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)\n",
    "\n",
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features_hot,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "estimator_hot.evaluate(input_fn=test_input_fn, steps=None)\n",
    "\n",
    "def getPrediction_hot(in_sentences):\n",
    "  labels = [\"NOT_LIBOR\", \"LIBOR\"]\n",
    "  input_examples_hot = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "  input_features_hot = run_classifier.convert_examples_to_features(input_examples_hot, label_list_hot, MAX_SEQ_LENGTH, tokenizer)\n",
    "  predict_input_fn = run_classifier.input_fn_builder(features=input_features_hot, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "  predictions_hot = estimator_hot.predict(predict_input_fn)\n",
    "  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions_hot)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-02T12:30:44.225743Z",
     "start_time": "2019-06-02T08:43:38.127336Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: \"cp\" command does not support provider-only URLs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58aa60ad1b243eab475c67739bcacfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(VBox(children=(HBox(children=(RadioButtons(description='Model:', index=1, options…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf53ae991f564fbf92134d58c376e647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid grey', height='400', width='400'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import appmode\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ipywidgets import Output\n",
    "from ipywidgets import Button, Layout\n",
    "from IPython.display import Javascript\n",
    "import pandas as pd\n",
    "from IPython.display import display as display\n",
    "from IPython.display import HTML\n",
    "from ipywidgets import HBox, VBox\n",
    "from collections import OrderedDict\n",
    "pd.options.display.html.table_schema = True\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = 10\n",
    "pd.options.display.max_colwidth = 300\n",
    "pd.options.display.width = 600\n",
    "pd.options.display.expand_frame_repr = True\n",
    "import nltk\n",
    "from nltk import SnowballStemmer\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from io import StringIO\n",
    "import csv\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "from bs4 import BeautifulSoup as BeautifulSoup\n",
    "from openpyxl import Workbook as Workbook\n",
    "import textract\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm, metrics\n",
    "import googleapiclient.discovery\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import auth as google_auth\n",
    "    google_auth.authenticate_user()\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] =  '/Users/josiasdewey/Downloads/Google Vision-e79587625a9d.json'\n",
    "service = googleapiclient.discovery.build('ml', 'v1')\n",
    "\n",
    "#Uncomment the below code if it is necessary to run predictions locally.  \n",
    "\n",
    "df_f = pd.read_excel('/Users/josiasdewey/Google_Drive/Project_Sunset/merged.xlsx')\n",
    "df_f = df_f.sample(frac=1, axis=1).reset_index(drop=True)\n",
    "df_f = df_f[pd.notnull(df_f['Text'])]\n",
    "df_f = df_f.drop_duplicates('Text')\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(df_f['Text'], df_f['Label'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "clf = LinearSVC()\n",
    "pipeline2 = Pipeline([('count_vect', count_vect), ('tfidf_transformer', tfidf_transformer), ('clf', clf)])\n",
    "model2 = pipeline2.fit(X_train_1, y_train_1)\n",
    "labels2 = df_f.drop_duplicates('Label')\n",
    "joblib.dump(pipeline2, 'model_type_libor.joblib')\n",
    "!gsutil cp ./model.joblib gs://$BUCKET_NAME/model.joblib\n",
    "\n",
    "'''\n",
    "df = pd.read_excel('/Users/josiasdewey/Google_Drive/Project_Sunset/hotdog3_g.xlsx')\n",
    "df = df.sample(frac=1, axis=1).reset_index(drop=True)\n",
    "df = df[pd.notnull(df['Text'])]\n",
    "df = df.drop_duplicates('Text')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Label'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "clf = LinearSVC()\n",
    "pipeline = Pipeline([('count_vect', count_vect), ('tfidf_transformer', tfidf_transformer), ('clf', clf)])\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "labels = df.drop_duplicates('Label')\n",
    "joblib.dump(pipeline, 'model_libor.joblib')\n",
    "'''\n",
    "\n",
    "def html_from_file_no_tags(file_path):\n",
    "    with open(file_path, 'rb') as myfile:\n",
    "        raw_text = myfile.read()\n",
    "    bsObj = BeautifulSoup(raw_text, 'lxml').text\n",
    "    return bsObj\n",
    "\n",
    "def remove_roman_numerals(text):\n",
    "    clean = re.sub('\\s*[ivx]+\\s+', ' ', text)\n",
    "    return clean\n",
    "\n",
    "def normalize_sent(sent, lower=False, lemm=False):\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    sent = re.sub(r'[^a-zA-Z\\s]', '', sent, re.I | re.A)\n",
    "    sent = sent.strip()\n",
    "    tokens = wpt.tokenize(sent)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "    if lemm == True:\n",
    "        filtered_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    sent = ' '.join(filtered_tokens)\n",
    "    sent = remove_roman_numerals(sent)\n",
    "    if lower == True:\n",
    "        sent = sent.lower()\n",
    "    return sent\n",
    "\n",
    "def load_files_from_directory(path):\n",
    "    list_of_text = []\n",
    "    for file in os.listdir(path):\n",
    "        filename = os.fsdecode(file)\n",
    "        file_path = path + filename\n",
    "        text = load_raw(file_path)\n",
    "        text = str(text)\n",
    "        list_of_text.append(text)\n",
    "\n",
    "def load(path, tags=False):\n",
    "    if path.endswith('html'):\n",
    "        if (tags):\n",
    "            return html_from_file_tags(path)\n",
    "        else:\n",
    "            return html_from_file_no_tags(path)\n",
    "    elif path.endswith('.txt'):\n",
    "        return str(text_from_file(path))\n",
    "    else:\n",
    "        text = text_from_binary(path)\n",
    "        return text\n",
    "        #except:\n",
    "         #   print('Failed to load as binary. Try reader that accepts url as argument (e.g., html_from_web_tags(url) or html_from_web_no_tags(url)).')\n",
    "\n",
    "def text_from_binary(file_path):\n",
    "    text = textract.process(file_path, method='tesseract', language='eng')\n",
    "    #text = 'Not set up for binary'\n",
    "    return text.decode('utf-8')#, 'ignore').strip()\n",
    "\n",
    "def html_from_file_tags(file_path):\n",
    "    with open(file_path, 'rb') as myfile:\n",
    "        raw_text = myfile.read()\n",
    "    return raw_text\n",
    "\n",
    "def text_from_file(file_path):\n",
    "    with open(file_path, 'rb') as myfile:\n",
    "        raw_text = myfile.read()\n",
    "    return raw_text\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    joined_text = \" \".join(stems)\n",
    "    print(joined_text)\n",
    "    return joined_text\n",
    "\n",
    "def create_progress_bar():\n",
    "    in_progress = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    visible=False,\n",
    "    description='Status:',\n",
    "    bar_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    orientation='horizontal'\n",
    ")\n",
    "    return in_progress\n",
    "\n",
    "def change_progress_bar(name, percent_complete):\n",
    "    name.value = percent_complete\n",
    "\n",
    "def is_progress_bar_visible(name, visible):\n",
    "    name.visible = visible\n",
    "    \n",
    "def clear_output(name):\n",
    "    name.close\n",
    "    \n",
    "def create_submit_button():\n",
    "    button = widgets.Button(\n",
    "    description='Submit',\n",
    "    disabled=False\n",
    "    )\n",
    "    button.style.button_color = 'steelblue'\n",
    "    button.layout={'border': '1px solid grey'}\n",
    "    return button\n",
    "\n",
    "def create_download_button():\n",
    "    button = widgets.Button(\n",
    "    description='Download Excel',\n",
    "    disabled=False\n",
    "    )\n",
    "    button.style.button_color = 'Honeydew'\n",
    "    button.layout={'border': '1px solid grey'}\n",
    "    return button\n",
    "\n",
    "def create_refresh_button():\n",
    "    button = widgets.Button(\n",
    "    description='Clear Output',\n",
    "    disabled=False\n",
    "    )\n",
    "    button.style.button_color = 'LightSlateGray'\n",
    "    button.layout={'border': '1px solid grey'}\n",
    "    return button\n",
    "\n",
    "#TODO Create seperate lists for individual items for other subject matters\n",
    "#TODO Add training back-end\n",
    "\n",
    "results = ''\n",
    "out = widgets.Output(layout=Layout(width=\"400\", height=\"400\", border='1px solid grey'))\n",
    "out_aux = widgets.Output()\n",
    "\n",
    "model_selection = widgets.RadioButtons(\n",
    "    options=['Bidirectional LTSM', 'Support Vector (SVC)', 'Gensim', 'MultinomialNB', 'Logistic Regression'],\n",
    "     value='Support Vector (SVC)',\n",
    "    description='Model:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "Pre_Processing_selection_lower = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Lowercase text',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='lowercase text',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "Pre_Processing_selection_lemm = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Lemmatize tokens',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='lemmatize',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "Pre_Processing_selection_stop = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Filter stop words',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='filter stop words',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "Pre_Processing_selection_small = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Filter single char tokens',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Remove one chars',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "file_path_text = widgets.Text(              \n",
    "    value='credit.txt',\n",
    "    placeholder='filename',\n",
    "    description='Filename:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "directory_path = widgets.Text(\n",
    "value='/Users/josiasdewey/Downloads/credit/',\n",
    "placeholder='path',\n",
    "description='Source Path:')\n",
    "\n",
    "df_path = widgets.Text(\n",
    "value='/Users/josiasdewey/Downloads/libor_2018_19.xlsx',\n",
    "placeholder='/Users/josiasdewey/Downloads/libor_2018_19.xlsx',\n",
    "description='Source Path:')\n",
    "\n",
    "save_file_path = widgets.Text(              \n",
    "    value='model.pickle',\n",
    "    placeholder='Filename',\n",
    "    description='Save:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "model_training_material = widgets.Text(              \n",
    "    value='training.xlsx',\n",
    "    placeholder='File Path',\n",
    "    description='Train Docs:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "clause_selection = widgets.SelectMultiple(\n",
    "    options=['All', 'MAC_definition', 'NOT_LIBOR', 'LIBOR', 'commitment_fee', 'change_control', 'change_control_prepayment', 'debt_issuance_prepayment', 'default_interest', 'disposition_assets_prepayment', 'equity_issuance_prepayment', 'eurodollar_rate', 'excess_cash_flow', 'fixed_charge_ratio', 'lender_inspection_rights', 'margin_leverage_ratio', 'margin_rating', 'monetary_default', 'net_worth', 'restricted_payments', 'Amendments_Consent', 'secured_facility', 'Amendments_Consent', 'unused_fee', 'voluntary_prepayment'],\n",
    "    value=['All'],\n",
    "    rows=10,\n",
    "    description='Clause selection:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "button = create_submit_button() \n",
    "download_button = create_download_button()\n",
    "refresh_button = create_refresh_button()\n",
    "tab1 = VBox(children=[HBox(children=[file_path_text, directory_path, df_path])])\n",
    "tab2 = VBox(children=[HBox(children=[model_selection, VBox(children=[Pre_Processing_selection_lower, Pre_Processing_selection_stop, Pre_Processing_selection_small, Pre_Processing_selection_lemm]), clause_selection])])\n",
    "tab3 = VBox(children=[HBox(children=[model_selection, model_training_material, save_file_path])])\n",
    "tab = widgets.Tab(children=[tab2, tab1, tab3])\n",
    "tab.set_title(0, 'Configure')\n",
    "tab.set_title(1, 'Predict')\n",
    "tab.set_title(2, 'Training')\n",
    "HBox_classify = HBox(children=[button, refresh_button, download_button])\n",
    "VBox_classify = VBox(children=[tab, HBox_classify])\n",
    "VBox_classify.layout={'border': '.25px solid grey'}\n",
    "\n",
    "#with out_aux:\n",
    "display(VBox_classify)\n",
    "\n",
    "returned_values = {} \n",
    "\n",
    "def clear_output():\n",
    "    out.clear_output()\n",
    "\n",
    "@download_button.on_click\n",
    "def download_button_clicked(b):\n",
    "    output = returned_values['df_output']\n",
    "    writer = pd.ExcelWriter('classification_results.xlsx')\n",
    "    output.to_excel(writer,'Sheet1')\n",
    "    writer.save()\n",
    "    download_button.description = 'Download complete'\n",
    "\n",
    "@refresh_button.on_click\n",
    "def refresh_button_clicked(b):\n",
    "    out.clear_output()\n",
    "    download_button.description = 'Download Excel'\n",
    "\n",
    "@out.capture()\n",
    "@button.on_click\n",
    "def on_button_clicked(b):\n",
    "#if 1==1:\n",
    "    clear_output()\n",
    "    download_button.description = 'Download Excel'\n",
    "    in_progress = create_progress_bar()\n",
    "    display(in_progress)\n",
    "    change_progress_bar(in_progress, 1)\n",
    "    input_path = '/Users/josiasdewey/Projects/test/'#Loan Agreement - NGP VI Greensboro (4827-4529-3719 v3).doc'\n",
    "    doc_list=[]\n",
    "    master = []\n",
    "    master_raw = []\n",
    "    category = []\n",
    "    x = 0\n",
    "    #completed_files = []\n",
    "    #df_c = pd.read_excel('/Users/josiasdewey/Projects/hknotebook/outputs2741671584.html_134.xlsx')\n",
    "    #for index, row in df_c.iterrows():\n",
    "        #completed_files.append(df_c['file'][index])\n",
    "    change_progress_bar(in_progress, 2)\n",
    "    for file in os.listdir(input_path):\n",
    "        filename = os.fsdecode(file)\n",
    "        print(filename)\n",
    "        file_path = input_path + filename\n",
    "        print(file_path)\n",
    "        change_progress_bar(in_progress, 3)\n",
    "        if filename.endswith('docx'):# and filename not in completed_files:\n",
    "            clean = []\n",
    "            raw = []\n",
    "            text = load(file_path)\n",
    "            text = str(text)\n",
    "            segment_sentences = sent_tokenize(text)\n",
    "            change_progress_bar(in_progress, 4)\n",
    "            for sent in segment_sentences:\n",
    "                tmp = []\n",
    "                cleaned = normalize_sent(sent)\n",
    "                tmp.append(cleaned)\n",
    "                name = 'projects/{}/models/{}/versions/{}'.format('elite-mix-229322', 'LIBOR23', 'v01', cache_discovery=False)\n",
    "                response = service.projects().predict(\n",
    "                name=name,\n",
    "                body={'instances': tmp}\n",
    "                ).execute()\n",
    "                change_progress_bar(in_progress, 5)\n",
    "                print(response['predictions'][0])\n",
    "                if response['predictions'][0] == 'LIBOR':\n",
    "                    print(sent)\n",
    "                    raw.append(sent)\n",
    "                    clean.append(cleaned)\n",
    "            #pred_sentences = sent_tokenize(text)\n",
    "            if 1 == 1:\n",
    "                change_progress_bar(in_progress, 6)\n",
    "                print(len(raw))\n",
    "                if 1 == 1:\n",
    "                    predictions = getPrediction_hot(raw)\n",
    "                    print(len(predictions))\n",
    "                    text_list = []\n",
    "                    cleaned_list = []\n",
    "                    change_progress_bar(in_progress, 7)\n",
    "                    for pred in predictions:\n",
    "                        if pred[2] == 'LIBOR' and len(pred[0]) > 100:\n",
    "                            #print(pred[1], pred[2], raw[i])\n",
    "                            print(pred[0])\n",
    "                            print(pred)\n",
    "                            text_list.append(pred[0])\n",
    "                            cleaned_list.append(normalize_sent(pred[0]))\n",
    "                        change_progress_bar(in_progress, 8)\n",
    "                    raw_text = ''.join(text_list)\n",
    "                    raw_text = raw_text.split()\n",
    "                    raw_text = ' '.join(raw_text)\n",
    "                    hand_off = ''.join(cleaned_list)\n",
    "                    #print(hand_off)\n",
    "                    master.append(hand_off)\n",
    "                    master_raw.append(raw_text)\n",
    "                    arrayify = [hand_off]\n",
    "                    #arrayify.append(hand_off)\n",
    "                    change_progress_bar(in_progress, 8)\n",
    "                    pred2 = model2.predict(arrayify)\n",
    "                    print(pred2[0])\n",
    "                    category.append(pred2[0])\n",
    "                    change_progress_bar(in_progress, 9)\n",
    "                    doc_list.append(filename)\n",
    "                    df = pd.DataFrame()                                                           \n",
    "                    df['text'] = master\n",
    "                    df['raw'] = master_raw\n",
    "                    df['file'] = doc_list\n",
    "                    df['category'] = category\n",
    "                    df.to_excel('outputs' + filename + '_' + str(x) + '.xlsx')\n",
    "                    change_progress_bar(in_progress, 10)\n",
    "                #except Exception as e: \n",
    "                    #print(e)\n",
    "                x=x+1\n",
    "    df_output = df\n",
    "    returned_values['df_output'] = df_output \n",
    "    in_progress.close()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
    "    with out:\n",
    "        df_output\n",
    "out_aux\n",
    "out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_progress_bar():\n",
    "    in_progress = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    visible=False,\n",
    "    description='Status:',\n",
    "    bar_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    orientation='horizontal'\n",
    "    )\n",
    "    return in_progress\n",
    "\n",
    "def change_progress_bar(name, percent_complete):\n",
    "    name.value = percent_complete\n",
    "\n",
    "def is_progress_bar_visible(name, visible):\n",
    "    name.visible = visible\n",
    "    \n",
    "def clear_output(name):\n",
    "    name.close\n",
    "    \n",
    "def create_submit_button():\n",
    "    button = widgets.Button(\n",
    "    description='Submit',\n",
    "    disabled=False\n",
    "    )\n",
    "    button.style.button_color = 'steelblue'\n",
    "    button.layout={'border': '1px solid grey'}\n",
    "    return button\n",
    "\n",
    "def create_download_button():\n",
    "    button = widgets.Button(\n",
    "    description='Download Excel',\n",
    "    disabled=False\n",
    "    )\n",
    "    button.style.button_color = 'Honeydew'\n",
    "    button.layout={'border': '1px solid grey'}\n",
    "    return button\n",
    "\n",
    "def create_refresh_button():\n",
    "    button = widgets.Button(\n",
    "    description='Clear Output',\n",
    "    disabled=False\n",
    "    )\n",
    "    button.style.button_color = 'LightSlateGray'\n",
    "    button.layout={'border': '1px solid grey'}\n",
    "    return button\n",
    "\n",
    "#TODO Create seperate lists for individual items for other subject matters\n",
    "#TODO Add training back-end\n",
    "\n",
    "results = ''\n",
    "out = widgets.Output(layout=Layout(width=\"400\", height=\"400\", border='1px solid grey'))\n",
    "out_aux = widgets.Output()\n",
    "\n",
    "model_selection = widgets.RadioButtons(\n",
    "    options=['Bidirectional LTSM', 'Support Vector (SVC)', 'Gensim', 'MultinomialNB', 'Logistic Regression'],\n",
    "     value='Support Vector (SVC)',\n",
    "    description='Model:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "Pre_Processing_selection_lower = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Lowercase text',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='lowercase text',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "Pre_Processing_selection_lemm = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Lemmatize tokens',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='lemmatize',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "Pre_Processing_selection_stop = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Filter stop words',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='filter stop words',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "Pre_Processing_selection_small = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Filter single char tokens',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Remove one chars',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "file_path_text = widgets.Text(              \n",
    "    value='credit.txt',\n",
    "    placeholder='filename',\n",
    "    description='Filename:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "directory_path = widgets.Text(\n",
    "value='/Users/josiasdewey/Downloads/credit/',\n",
    "placeholder='path',\n",
    "description='Source Path:')\n",
    "\n",
    "df_path = widgets.Text(\n",
    "value='/Users/josiasdewey/Downloads/libor_2018_19.xlsx',\n",
    "placeholder='/Users/josiasdewey/Downloads/libor_2018_19.xlsx',\n",
    "description='Source Path:')\n",
    "\n",
    "save_file_path = widgets.Text(              \n",
    "    value='model.pickle',\n",
    "    placeholder='Filename',\n",
    "    description='Save:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "model_training_material = widgets.Text(              \n",
    "    value='training.xlsx',\n",
    "    placeholder='File Path',\n",
    "    description='Train Docs:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "clause_selection = widgets.SelectMultiple(\n",
    "    options=['All', 'MAC_definition', 'NOT_LIBOR', 'LIBOR', 'commitment_fee', 'change_control', 'change_control_prepayment', 'debt_issuance_prepayment', 'default_interest', 'disposition_assets_prepayment', 'equity_issuance_prepayment', 'eurodollar_rate', 'excess_cash_flow', 'fixed_charge_ratio', 'lender_inspection_rights', 'margin_leverage_ratio', 'margin_rating', 'monetary_default', 'net_worth', 'restricted_payments', 'Amendments_Consent', 'secured_facility', 'Amendments_Consent', 'unused_fee', 'voluntary_prepayment'],\n",
    "    value=['All'],\n",
    "    rows=10,\n",
    "    description='Clause selection:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "button = create_submit_button() \n",
    "download_button = create_download_button()\n",
    "refresh_button = create_refresh_button()\n",
    "tab1 = VBox(children=[HBox(children=[file_path_text, directory_path, df_path])])\n",
    "tab2 = VBox(children=[HBox(children=[model_selection, VBox(children=[Pre_Processing_selection_lower, Pre_Processing_selection_stop, Pre_Processing_selection_small, Pre_Processing_selection_lemm]), clause_selection])])\n",
    "tab3 = VBox(children=[HBox(children=[model_selection, model_training_material, save_file_path])])\n",
    "tab = widgets.Tab(children=[tab2, tab1, tab3])\n",
    "tab.set_title(0, 'Configure')\n",
    "tab.set_title(1, 'Predict')\n",
    "tab.set_title(2, 'Training')\n",
    "HBox_classify = HBox(children=[button, refresh_button, download_button])\n",
    "VBox_classify = VBox(children=[tab, HBox_classify])\n",
    "VBox_classify.layout={'border': '.25px solid grey'}\n",
    "VBox_classify\n",
    "\n",
    "with out_aux:\n",
    "    display(VBox_classify)\n",
    "\n",
    "returned_values = {} \n",
    "\n",
    "def clear_output():\n",
    "    out.clear_output()\n",
    "\n",
    "@download_button.on_click\n",
    "def download_button_clicked(b):\n",
    "    output = returned_values['df_output']\n",
    "    writer = pd.ExcelWriter('classification_results.xlsx')\n",
    "    output.to_excel(writer,'Sheet1')\n",
    "    writer.save()\n",
    "    download_button.description = 'Download complete'\n",
    "\n",
    "@refresh_button.on_click\n",
    "def refresh_button_clicked(b):\n",
    "    out.clear_output()\n",
    "    download_button.description = 'Download Excel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fd41036f42465287f4a3926d5e6ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Clause selection:', index=(0,), options=('All', 'MAC_definition', 'NOT_LIBOR', 'LI…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clause_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59d3009e6ae4823a343845236c2ac44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(VBox(children=(HBox(children=(RadioButtons(description='Model:', index=1, options…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VBox_classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "hide_input": false,
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "nteract": {
   "version": "0.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "217.717px",
    "left": "38px",
    "top": "123.867px",
    "width": "261.283px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
